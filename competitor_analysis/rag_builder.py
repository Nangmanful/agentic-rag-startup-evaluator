# ------------------------------------------------------------
# rag_builder.py
# RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• - Vector Store ìƒì„± ë° ê²€ìƒ‰
# ------------------------------------------------------------

import os
from typing import List, Optional
from pathlib import Path

from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document


class CompetitorRAGBuilder:
    """ê²½ìŸì‚¬ ë¶„ì„ì„ ìœ„í•œ RAG íŒŒì´í”„ë¼ì¸ ë¹Œë”"""

    def __init__(
        self,
        data_dir: str = "competitor_analysis/data",
        vector_store_path: Optional[str] = None,
        embedding_model: str = "text-embedding-3-small",
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        """
        RAG ë¹Œë” ì´ˆê¸°í™”

        Args:
            data_dir: PDF ë¬¸ì„œê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬
            vector_store_path: Vector Store ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ë©”ëª¨ë¦¬ì—ë§Œ ì €ì¥)
            embedding_model: OpenAI Embedding ëª¨ë¸ëª…
            chunk_size: ì²­í¬ í¬ê¸° (í† í° ìˆ˜)
            chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
        """
        self.data_dir = Path(data_dir)
        self.vector_store_path = vector_store_path
        self.embedding_model = embedding_model
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

        self.embeddings = OpenAIEmbeddings(model=embedding_model)
        self.vectorstore: Optional[FAISS] = None

    def load_documents(self) -> List[Document]:
        """
        ë°ì´í„° ë””ë ‰í† ë¦¬ì—ì„œ PDF ë¬¸ì„œ ë¡œë“œ

        Returns:
            ë¡œë“œëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        if not self.data_dir.exists():
            print(f"âš ï¸  ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.data_dir}")
            print(f"ğŸ“ ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
            self.data_dir.mkdir(parents=True, exist_ok=True)
            return []

        pdf_files = list(self.data_dir.glob("*.pdf"))

        if not pdf_files:
            print(f"âš ï¸  {self.data_dir}ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print(f"ğŸ“š {len(pdf_files)}ê°œì˜ PDF íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")

        documents = []
        for pdf_file in pdf_files[:4]:  # ìµœëŒ€ 4ê°œ ì œí•œ
            try:
                loader = PyPDFLoader(str(pdf_file))
                docs = loader.load()
                print(f"   âœ“ {pdf_file.name}: {len(docs)} í˜ì´ì§€")
                documents.extend(docs)
            except Exception as e:
                print(f"   âœ— {pdf_file.name} ë¡œë“œ ì‹¤íŒ¨: {e}")

        return documents

    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 

        Args:
            documents: ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

        Returns:
            ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )

        splits = text_splitter.split_documents(documents)
        print(f"âœ‚ï¸  {len(documents)} ë¬¸ì„œë¥¼ {len(splits)}ê°œ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

        return splits

    def build_vectorstore(self, documents: List[Document]) -> FAISS:
        """
        Vector Store êµ¬ì¶•

        Args:
            documents: ë¶„í• ëœ ë¬¸ì„œ ì²­í¬

        Returns:
            FAISS Vector Store
        """
        if not documents:
            print("âš ï¸  ë¬¸ì„œê°€ ì—†ì–´ ë¹ˆ Vector Storeë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            # ë¹ˆ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ë”ë¯¸ ë¬¸ì„œ ì‚¬ìš©)
            dummy_doc = Document(
                page_content="No industry reports available. Using web search only.",
                metadata={"source": "dummy"}
            )
            vectorstore = FAISS.from_documents([dummy_doc], self.embeddings)
        else:
            print(f"ğŸ”¨ Vector Storeë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤... (ì„ë² ë”© ìƒì„± ì¤‘)")
            vectorstore = FAISS.from_documents(documents, self.embeddings)
            print(f"âœ… Vector Store êµ¬ì¶• ì™„ë£Œ: {len(documents)} ì²­í¬")

        self.vectorstore = vectorstore
        return vectorstore

    def save_vectorstore(self):
        """Vector Storeë¥¼ ë””ìŠ¤í¬ì— ì €ì¥"""
        if self.vectorstore is None:
            print("âš ï¸  ì €ì¥í•  Vector Storeê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        if self.vector_store_path is None:
            print("âš ï¸  ì €ì¥ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ì—ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.")
            return

        save_path = Path(self.vector_store_path)
        save_path.parent.mkdir(parents=True, exist_ok=True)

        self.vectorstore.save_local(str(save_path))
        print(f"ğŸ’¾ Vector Store ì €ì¥ ì™„ë£Œ: {save_path}")

    def load_vectorstore(self) -> Optional[FAISS]:
        """ì €ì¥ëœ Vector Store ë¡œë“œ"""
        if self.vector_store_path is None:
            return None

        load_path = Path(self.vector_store_path)
        if not load_path.exists():
            print(f"âš ï¸  ì €ì¥ëœ Vector Storeê°€ ì—†ìŠµë‹ˆë‹¤: {load_path}")
            return None

        try:
            vectorstore = FAISS.load_local(
                str(load_path),
                self.embeddings,
                allow_dangerous_deserialization=True
            )
            print(f"âœ… Vector Store ë¡œë“œ ì™„ë£Œ: {load_path}")
            self.vectorstore = vectorstore
            return vectorstore
        except Exception as e:
            print(f"âŒ Vector Store ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def build_or_load(self, force_rebuild: bool = False) -> FAISS:
        """
        Vector Storeë¥¼ ë¹Œë“œí•˜ê±°ë‚˜ ë¡œë“œ

        Args:
            force_rebuild: Trueë©´ ê¸°ì¡´ Vector Storeë¥¼ ë¬´ì‹œí•˜ê³  ì¬ë¹Œë“œ

        Returns:
            FAISS Vector Store
        """
        # ê¸°ì¡´ Vector Store ë¡œë“œ ì‹œë„
        if not force_rebuild and self.vector_store_path:
            vectorstore = self.load_vectorstore()
            if vectorstore is not None:
                return vectorstore

        # ìƒˆë¡œ ë¹Œë“œ
        print("ğŸ—ï¸  Vector Storeë¥¼ ìƒˆë¡œ ë¹Œë“œí•©ë‹ˆë‹¤...")
        documents = self.load_documents()

        if documents:
            splits = self.split_documents(documents)
            vectorstore = self.build_vectorstore(splits)

            # ì €ì¥
            if self.vector_store_path:
                self.save_vectorstore()
        else:
            # ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ë”ë¯¸ Vector Store
            vectorstore = self.build_vectorstore([])

        return vectorstore

    def retrieve_context(
        self,
        query: str,
        k: int = 3,
        score_threshold: float = 0.5
    ) -> str:
        """
        ì¿¼ë¦¬ì— ëŒ€í•œ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜
            score_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0-1)

        Returns:
            ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        if self.vectorstore is None:
            print("âš ï¸  Vector Storeê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "No industry context available."

        try:
            # ìœ ì‚¬ë„ ê²€ìƒ‰
            docs_with_scores = self.vectorstore.similarity_search_with_score(
                query, k=k
            )

            # ì„ê³„ê°’ í•„í„°ë§
            relevant_docs = [
                doc for doc, score in docs_with_scores
                if score >= score_threshold
            ]

            if not relevant_docs:
                return "No highly relevant industry context found."

            # ì»¨í…ìŠ¤íŠ¸ ê²°í•©
            context = "\n\n---\n\n".join([
                f"[Source: {doc.metadata.get('source', 'Unknown')}]\n{doc.page_content}"
                for doc in relevant_docs
            ])

            return context

        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return "Error retrieving industry context."


# -----------------------------
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# -----------------------------
def initialize_rag(
    data_dir: str = "competitor_analysis/data",
    force_rebuild: bool = False
) -> CompetitorRAGBuilder:
    """
    RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ê°„í¸ í•¨ìˆ˜)

    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬
        force_rebuild: ê°•ì œ ì¬ë¹Œë“œ ì—¬ë¶€

    Returns:
        ì´ˆê¸°í™”ëœ RAG Builder
    """
    base_dir = Path(__file__).resolve().parent
    data_dir = Path(data_dir or "data")
    if not data_dir.is_absolute():
        data_dir = (base_dir / data_dir).resolve()

    vector_store_path = data_dir / "vector_store" / "faiss_index"

    rag_builder = CompetitorRAGBuilder(
        data_dir=data_dir,
        vector_store_path=vector_store_path,
        chunk_size=500,
        chunk_overlap=50
    )

    # Vector Store ë¹Œë“œ ë˜ëŠ” ë¡œë“œ
    rag_builder.build_or_load(force_rebuild=force_rebuild)

    return rag_builder


# -----------------------------
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# -----------------------------
if __name__ == "__main__":
    from dotenv import load_dotenv
    load_dotenv()

    print("="*80)
    print("ğŸ”§ RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    print("="*80 + "\n")

    # RAG ì´ˆê¸°í™”
    rag_builder = initialize_rag(force_rebuild=False)

    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
    test_query = "What are the key factors for evaluating medical AI startups?"
    print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {test_query}")
    print("-"*80)

    context = rag_builder.retrieve_context(test_query, k=2)
    print(f"\nğŸ“„ ê²€ìƒ‰ ê²°ê³¼:\n{context}\n")

    print("="*80)
    print("âœ… RAG í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    print("="*80)
